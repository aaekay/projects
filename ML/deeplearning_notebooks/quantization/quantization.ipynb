{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quatization is the process of converting large set of values into smaller one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would be quantizing hugging face models so that we can run them on low memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install below libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (4.42.3)\n",
      "Requirement already satisfied: filelock in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from transformers) (2.30.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from requests->transformers) (2024.6.2)\n",
      "Collecting quanto\n",
      "  Downloading quanto-0.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting torch>=2.2.0 (from quanto)\n",
      "  Downloading torch-2.3.1-cp39-cp39-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting ninja (from quanto)\n",
      "  Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from quanto) (1.25.2)\n",
      "Requirement already satisfied: safetensors in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from quanto) (0.4.3)\n",
      "Requirement already satisfied: filelock in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch>=2.2.0->quanto) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch>=2.2.0->quanto) (4.12.2)\n",
      "Collecting sympy (from torch>=2.2.0->quanto)\n",
      "  Downloading sympy-1.13.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=2.2.0->quanto)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch>=2.2.0->quanto) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch>=2.2.0->quanto) (2024.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.2.0->quanto)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.2.0->quanto)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.2.0->quanto)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.2.0->quanto)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.2.0->quanto)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.2.0->quanto)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.2.0->quanto)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.2.0->quanto)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.2.0->quanto)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.2.0->quanto)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.2.0->quanto)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.3.1 (from torch>=2.2.0->quanto)\n",
      "  Downloading triton-2.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.2.0->quanto) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from jinja2->torch>=2.2.0->quanto) (2.1.5)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch>=2.2.0->quanto)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading quanto-0.2.0-py3-none-any.whl (90 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.3.1-cp39-cp39-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading triton-2.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.0-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: ninja, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, quanto\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.2.0.103\n",
      "    Uninstalling nvidia-cusparse-cu12-12.2.0.103:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.2.0.103\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.4.107\n",
      "    Uninstalling nvidia-curand-cu12-10.3.4.107:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.4.107\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.0.12.1\n",
      "    Uninstalling nvidia-cufft-cu12-11.0.12.1:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.0.12.1\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.3.101\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.3.101:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.3.101\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.3.107\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.3.107:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.3.107\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.3.101\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.3.101:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.3.101\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.3.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.3.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.3.4.1\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.5.4.101\n",
      "    Uninstalling nvidia-cusolver-cu12-11.5.4.101:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.5.4.101\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.7.29\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.7.29:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.7.29\n",
      "Successfully installed mpmath-1.3.0 networkx-3.2.1 ninja-1.11.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 quanto-0.2.0 sympy-1.13.0 torch-2.3.1 triton-2.3.1\n",
      "Requirement already satisfied: torch in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: networkx in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install quanto\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EleutherAI is a non profit research lab, focused on interpretability, alignment, and ethics of AI. We would be importing this model from huggingface.\n",
    "What is hugging face?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaekay/miniconda3/envs/test2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model_name = \"EleutherAI/pythia-410m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is AutoModelForCausaLM? <br>\n",
    "HF provied a class where it automatically infers from the model name that what kind of config it would need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage = False) # to use low cpu mem usage we require accelerate,\n",
    "# we are keeping it false since we dont have ram limitations for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autotokenizer class would help in loading the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12092,   619,  1416,   310,  5533,    13,  5490,   209,   187,    42,\n",
      "          1353,   247,   747, 17782,   281,   253,  2165,    15]])\n",
      "Hello my name is bond, James \n",
      "I'm a newbie to the game.\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello my name is bond, James \"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "print(outputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's estimate the size of the model \\\n",
    "it is 410 Million Parameter with fp32, it would be 4 bytes for each resulting in 1.6 gb of model. \\\n",
    "The below function will help us to get the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def named_module_tensors(module, recurse=False):\n",
    "    for named_parameter in module.named_parameters(recurse=recurse):\n",
    "      name, val = named_parameter\n",
    "      flag = True\n",
    "      if hasattr(val,\"_data\") or hasattr(val,\"_scale\"):\n",
    "        if hasattr(val,\"_data\"):\n",
    "          yield name + \"._data\", val._data\n",
    "        if hasattr(val,\"_scale\"):\n",
    "          yield name + \"._scale\", val._scale\n",
    "      else:\n",
    "        yield named_parameter\n",
    "\n",
    "    for named_buffer in module.named_buffers(recurse=recurse):\n",
    "      yield named_buffer\n",
    "\n",
    "def dtype_byte_size(dtype):\n",
    "    \"\"\"\n",
    "    Returns the size (in bytes) occupied by one parameter of type `dtype`.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    if dtype == torch.bool:\n",
    "        return 1 / 8\n",
    "    bit_search = re.search(r\"[^\\d](\\d+)$\", str(dtype))\n",
    "    if bit_search is None:\n",
    "        raise ValueError(f\"`dtype` is not a valid dtype: {dtype}.\")\n",
    "    bit_size = int(bit_search.groups()[0])\n",
    "    return bit_size // 8\n",
    "def compute_module_sizes(model):\n",
    "    \"\"\"\n",
    "    Compute the size of each submodule of a given model.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    module_sizes = defaultdict(int)\n",
    "    for name, tensor in named_module_tensors(model, recurse=True):\n",
    "      size = tensor.numel() * dtype_byte_size(tensor.dtype)\n",
    "      name_parts = name.split(\".\")\n",
    "      for idx in range(len(name_parts) + 1):\n",
    "        module_sizes[\".\".join(name_parts[:idx])] += size\n",
    "\n",
    "    return module_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6402112960000002 GB\n"
     ]
    }
   ],
   "source": [
    "model_size = compute_module_sizes(model)\n",
    "print(model_size[\"\"]*1e-9, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.0\n"
     ]
    }
   ],
   "source": [
    "from quanto import freeze, quantize\n",
    "# quantize(model, weights=torch.int8, activations=None) # for older qunato it would work\n",
    "import quanto\n",
    "print(quanto.__version__)\n",
    "# version 0.2 uses quanto.qint8\n",
    "quantize(model, weights=quanto.qint8, activations=None)\n",
    "# call freeze it, the float weights would be replace by linear weights\n",
    "freeze(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.580794472 GB\n"
     ]
    }
   ],
   "source": [
    "model_size = compute_module_sizes(model)\n",
    "print(model_size[\"\"]*1e-9, \"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the output of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12092,   619,  1416,   310,  5533,    13,  5490,   209,   187,   395,\n",
      "           309,   717,   247,   747, 17782,   275,   253,  2165]])\n",
      "Hello my name is bond, James \n",
      "and I am a newbie in the game\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "print(outputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
